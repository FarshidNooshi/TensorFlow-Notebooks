{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/https-deeplearning-ai/tensorflow-1-public/blob/main/C4/W2/ungraded_labs/C4_W2_Lab_2_single_layer_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJpXaP2ntTqX"
   },
   "source": [
    "# Ungraded Lab: Training a Single Layer Neural Network with Time Series Data\n",
    "\n",
    "Now that you've seen statistical methods in the previous week, you will now shift to using neural networks to build your prediction models. You will start with a simple network in this notebook and move on to more complex architectures in the next weeks. By the end of this lab, you will be able to:\n",
    "\n",
    "* build a single layer network and train it using the same synthetic data you used in the previous lab\n",
    "* prepare time series data for training and evaluation\n",
    "* measure the performance of your model against a validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fhiJItrZDdqo"
   },
   "source": [
    "## Imports\n",
    "\n",
    "You will first import the packages you will need to execute all the code in this lab. You will use:\n",
    "* [Tensorflow](https://www.tensorflow.org/api_docs/python/tf) to build your model and prepare data windows\n",
    "* [Numpy](https://numpy.org/) for numerical processing\n",
    "* and Matplotlib's [PyPlot](https://matplotlib.org/3.5.1/api/_as_gen/matplotlib.pyplot.html) library for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BOjujz601HcS"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pP77XGmrqLxP"
   },
   "source": [
    "## Utilities\n",
    "\n",
    "You will then define some utility functions that you also saw in the previous labs. These will take care of visualizing your time series data and model predictions, as well as generating the synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yw5jEYuBADvA"
   },
   "outputs": [],
   "source": [
    "def plot_series(time, series, format=\"-\", start=0, end=None):\n",
    "    \"\"\"\n",
    "    Visualizes time series data\n",
    "\n",
    "    Args:\n",
    "      time (array of int) - contains the time steps\n",
    "      series (array of int) - contains the measurements for each time step\n",
    "      format - line style when plotting the graph\n",
    "      label - tag for the line\n",
    "      start - first time step to plot\n",
    "      end - last time step to plot\n",
    "    \"\"\"\n",
    "\n",
    "    # Setup dimensions of the graph figure\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    if type(series) is tuple:\n",
    "\n",
    "      for series_num in series:\n",
    "        # Plot the time series data\n",
    "        plt.plot(time[start:end], series_num[start:end], format)\n",
    "\n",
    "    else:\n",
    "      # Plot the time series data\n",
    "      plt.plot(time[start:end], series[start:end], format)\n",
    "\n",
    "    # Label the x-axis\n",
    "    plt.xlabel(\"Time\")\n",
    "\n",
    "    # Label the y-axis\n",
    "    plt.ylabel(\"Value\")\n",
    "\n",
    "    # Overlay a grid on the graph\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Draw the graph on screen\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def trend(time, slope=0):\n",
    "    \"\"\"\n",
    "    Generates synthetic data that follows a straight line given a slope value.\n",
    "\n",
    "    Args:\n",
    "      time (array of int) - contains the time steps\n",
    "      slope (float) - determines the direction and steepness of the line\n",
    "\n",
    "    Returns:\n",
    "      series (array of float) - measurements that follow a straight line\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute the linear series given the slope\n",
    "    series = slope * time\n",
    "\n",
    "    return series\n",
    "\n",
    "\n",
    "def seasonal_pattern(season_time):\n",
    "    \"\"\"\n",
    "    Just an arbitrary pattern, you can change it if you wish\n",
    "    \n",
    "    Args:\n",
    "      season_time (array of float) - contains the measurements per time step\n",
    "\n",
    "    Returns:\n",
    "      data_pattern (array of float) -  contains revised measurement values according \n",
    "                                  to the defined pattern\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate the values using an arbitrary pattern\n",
    "    data_pattern = np.where(season_time < 0.4,\n",
    "                    np.cos(season_time * 2 * np.pi),\n",
    "                    1 / np.exp(3 * season_time))\n",
    "    \n",
    "    return data_pattern\n",
    "\n",
    "\n",
    "def seasonality(time, period, amplitude=1, phase=0):\n",
    "    \"\"\"\n",
    "    Repeats the same pattern at each period\n",
    "\n",
    "    Args:\n",
    "      time (array of int) - contains the time steps\n",
    "      period (int) - number of time steps before the pattern repeats\n",
    "      amplitude (int) - peak measured value in a period\n",
    "      phase (int) - number of time steps to shift the measured values\n",
    "\n",
    "    Returns:\n",
    "      data_pattern (array of float) - seasonal data scaled by the defined amplitude\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the measured values per period\n",
    "    season_time = ((time + phase) % period) / period\n",
    "\n",
    "    # Generates the seasonal data scaled by the defined amplitude\n",
    "    data_pattern = amplitude * seasonal_pattern(season_time)\n",
    "\n",
    "    return data_pattern\n",
    "\n",
    "\n",
    "def noise(time, noise_level=1, seed=None):\n",
    "    \"\"\"Generates a normally distributed noisy signal\n",
    "\n",
    "    Args:\n",
    "      time (array of int) - contains the time steps\n",
    "      noise_level (float) - scaling factor for the generated signal\n",
    "      seed (int) - number generator seed for repeatability\n",
    "\n",
    "    Returns:\n",
    "      noise (array of float) - the noisy signal\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the random number generator\n",
    "    rnd = np.random.RandomState(seed)\n",
    "\n",
    "    # Generate a random number for each time step and scale by the noise level\n",
    "    noise = rnd.randn(len(time)) * noise_level\n",
    "    \n",
    "    return noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1K6pkNAr02r"
   },
   "source": [
    "## Generate the Synthetic Data\n",
    "\n",
    "The code below generates the same synthetic data you used in the previous lab. It will contain 1,461 data points that has trend, seasonality, and noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zswl7jRtGzkk"
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "time = np.arange(4 * 365 + 1, dtype=\"float32\")\n",
    "baseline = 10\n",
    "amplitude = 40\n",
    "slope = 0.05\n",
    "noise_level = 5\n",
    "\n",
    "# Create the series\n",
    "series = baseline + trend(time, slope) + seasonality(time, period=365, amplitude=amplitude)\n",
    "\n",
    "# Update with noise\n",
    "series += noise(time, noise_level, seed=42)\n",
    "\n",
    "# Plot the results\n",
    "plot_series(time, series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_73LFoMr91G"
   },
   "source": [
    "## Split the Dataset\n",
    "\n",
    "Next up, you will split the data above into training and validation sets. You will take the first 1,000 points for training while the rest is for validation,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L_0TYTNxBOVi"
   },
   "outputs": [],
   "source": [
    "# Define the split time\n",
    "split_time = 1000\n",
    "\n",
    "# Get the train set \n",
    "time_train = time[:split_time]\n",
    "x_train = series[:split_time]\n",
    "\n",
    "# Get the validation set\n",
    "time_valid = time[split_time:]\n",
    "x_valid = series[split_time:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GqCx0mm_mPg"
   },
   "source": [
    "You can inspect these sets visually by using the same utility function for plotting. Notice that in general, the validation set has higher values (i.e. y-axis) than those in the training set. Your model should be able to predict those values just by learning from the trend and seasonality of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3e3Fo-TpBYvz"
   },
   "outputs": [],
   "source": [
    "# Plot the train set\n",
    "plot_series(time_train, x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U3b_u8t5BlxQ"
   },
   "outputs": [],
   "source": [
    "# Plot the validation set\n",
    "plot_series(time_valid, x_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-kFZga8vnHF"
   },
   "source": [
    "## Prepare features and labels\n",
    "\n",
    "You will then prepare your data windows as shown in the previous lab. It is good to declare parameters in a separate cell so you can easily tweak it later if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "olwcPlwfBWqC"
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "window_size = 20\n",
    "batch_size = 32\n",
    "shuffle_buffer_size = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rMok0QE5EEB5"
   },
   "source": [
    "The following function contains all the preprocessing steps you did in the previous lab. This makes it modular so you can easily use it in your other projects if needed. \n",
    "\n",
    "One thing to note here is the `window_size + 1` when you call `dataset.window()`. There is a `+ 1` to indicate that you're taking the next point as the label. For example, the first 20 points will be the feature so the 21st point will be the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4sTTIOCbyShY"
   },
   "outputs": [],
   "source": [
    "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
    "    \"\"\"Generates dataset windows\n",
    "\n",
    "    Args:\n",
    "      series (array of float) - contains the values of the time series\n",
    "      window_size (int) - the number of time steps to include in the feature\n",
    "      batch_size (int) - the batch size\n",
    "      shuffle_buffer(int) - buffer size to use for the shuffle method\n",
    "\n",
    "    Returns:\n",
    "      dataset (TF Dataset) - TF Dataset containing time windows\n",
    "    \"\"\"\n",
    "  \n",
    "    # Generate a TF Dataset from the series values\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(series)\n",
    "    \n",
    "    # Window the data but only take those with the specified size\n",
    "    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n",
    "    \n",
    "    # Flatten the windows by putting its elements in a single batch\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n",
    "\n",
    "    # Create tuples with features and labels \n",
    "    dataset = dataset.map(lambda window: (window[:-1], window[-1]))\n",
    "\n",
    "    # Shuffle the windows\n",
    "    dataset = dataset.shuffle(shuffle_buffer)\n",
    "    \n",
    "    # Create batches of windows\n",
    "    dataset = dataset.batch(batch_size).prefetch(1)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ar-5QbIGT5I"
   },
   "source": [
    "Now you can generate the dataset windows from the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "55UeIZDoCQWj"
   },
   "outputs": [],
   "source": [
    "# Generate the dataset windows\n",
    "dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FzQ6DCfeGcKT"
   },
   "source": [
    "You can again inspect the output to see if the function is behaving as expected. The code below will use the [`take()`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#take) method of the `tf.data.Dataset` API to grab a single batch. It will then print several properties of this batch such as the data type and shape of the elements. As expected, it should have a 2-element tuple (i.e. `(feature, label)`) and the shapes of these should align with the batch and window sizes you declared earlier which are `32` and `20` by default, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CWnysDAz04Ad"
   },
   "outputs": [],
   "source": [
    "# Print properties of a single batch\n",
    "for windows in dataset.take(1):\n",
    "  print(f'data type: {type(windows)}')\n",
    "  print(f'number of elements in the tuple: {len(windows)}')\n",
    "  print(f'shape of first element: {windows[0].shape}')\n",
    "  print(f'shape of second element: {windows[1].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1rN2EsF01QM"
   },
   "source": [
    "## Build and compile the model\n",
    "\n",
    "Next, you will build the single layer neural network. This will just be a one-unit [Dense](https://keras.io/api/layers/core_layers/dense/) layer as shown below. You will assign the layer to a variable `l0` so you can also look at the final weights later using the [`get_weights()`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer#get_weights) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ou-WmE2AXu6B"
   },
   "outputs": [],
   "source": [
    "# Build the single layer neural network\n",
    "l0 = tf.keras.layers.Dense(1, input_shape=[window_size])\n",
    "model = tf.keras.models.Sequential([l0])\n",
    "\n",
    "# Print the initial layer weights\n",
    "print(\"Layer weights: \\n {} \\n\".format(l0.get_weights()))\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rm24sbttJxif"
   },
   "source": [
    "You will set [*mean squared error (mse)*](https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanSquaredError) as the loss function and use [*stochastic gradient descent (SGD)*](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD) to optimize the weights during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1l-iM-x-Jwep"
   },
   "outputs": [],
   "source": [
    "# Set the training parameters\n",
    "model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(learning_rate=1e-6, momentum=0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MD1kwa1E1yiQ"
   },
   "source": [
    "## Train the Model\n",
    "\n",
    "Now you can proceed to train your model. You will feed in the prepared data windows and run the training for 100 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tVJTpEUNCqAk"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(dataset,epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JSmov9j_d9Nt"
   },
   "source": [
    "You can see the final weights by again calling the `get_weights()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yua-kFOgDBxK"
   },
   "outputs": [],
   "source": [
    "# Print the layer weights\n",
    "print(\"Layer weights {}\".format(l0.get_weights()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TwzM0T2s2dXC"
   },
   "source": [
    "## Model Prediction\n",
    "\n",
    "With the training finished, you can now measure the performance of your model. You can generate a model prediction by passing a batch of data windows. If you will be slicing a window from the original `series` array, you will need to add a batch dimension before passing it to the model. That can be done by indexing with the [`np.newaxis`](https://numpy.org/doc/stable/reference/constants.html?highlight=newaxis#numpy.newaxis) constant or using the [`np.expand_dims()`](https://numpy.org/doc/stable/reference/generated/numpy.expand_dims.html) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cV7ZLzXaGSOz"
   },
   "outputs": [],
   "source": [
    "# Shape of the first 20 data points slice\n",
    "print(f'shape of series[0:20]: {series[0:20].shape}')\n",
    "\n",
    "# Shape after adding a batch dimension\n",
    "print(f'shape of series[0:20][np.newaxis]: {series[0:20][np.newaxis].shape}')\n",
    "\n",
    "# Shape after adding a batch dimension (alternate way)\n",
    "print(f'shape of series[0:20][np.newaxis]: {np.expand_dims(series[0:20], axis=0).shape}')\n",
    "\n",
    "# Sample model prediction\n",
    "print(f'model prediction: {model.predict(series[0:20][np.newaxis])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YYa_fKunkgqq"
   },
   "source": [
    "To compute the metrics, you will want to generate model predictions for your validation set. Remember that this set refers to points at index `1000` to `1460` of the entire series. You will need to code the steps to generate those from your model. The cell below demonstrates one way of doing that. \n",
    "\n",
    "Basically, it feeds the entire series to your model 20 points at a time and append all results to a `forecast` list. It will then slice the points that corresponds to the validation set. \n",
    "\n",
    "The slice index below is `split_time - window_size:` because the forecast list is smaller than the series by 20 points (i.e. the window size). Since the window size is 20, the first data point in the `forecast` list corresponds to the prediction for time at index `20`. You cannot make predictions at index `0` to `19` because those are smaller than the window size. Thus, when you slice with `split_time - window_size:`, you will be getting the points at the time indices that aligns with those in the validation set.\n",
    "\n",
    "*Note: You might notice that this cell takes a while to run. In the next two labs, you will see other approaches to generating predictions to make the code run faster. You might already have some ideas and feel free to try them out after completing this lab.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-gtVJuLVxR-M"
   },
   "outputs": [],
   "source": [
    "# Initialize a list\n",
    "forecast = []\n",
    "\n",
    "# Use the model to predict data points per window size\n",
    "for time in range(len(series) - window_size):\n",
    "  forecast.append(model.predict(series[time:time + window_size][np.newaxis]))\n",
    "\n",
    "# Slice the points that are aligned with the validation set\n",
    "forecast = forecast[split_time - window_size:]\n",
    "\n",
    "# Compare number of elements in the predictions and the validation set\n",
    "print(f'length of the forecast list: {len(forecast)}')\n",
    "print(f'shape of the validation set: {x_valid.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8MebvmkcP6ll"
   },
   "source": [
    "To visualize the results, you will need to convert the predictions to a form that the `plot_series()` utility function accepts. That involves converting the list to a numpy array and dropping the single dimensional axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "omfWb6EkMdRL"
   },
   "outputs": [],
   "source": [
    "# Preview shapes after using the conversion and squeeze methods\n",
    "print(f'shape after converting to numpy array: {np.array(forecast).shape}')\n",
    "print(f'shape after squeezing: {np.array(forecast).squeeze().shape}')\n",
    "\n",
    "# Convert to a numpy array and drop single dimensional axes\n",
    "results = np.array(forecast).squeeze()\n",
    "\n",
    "# Overlay the results with the validation set\n",
    "plot_series(time_valid, (x_valid, results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EcAbECXeV5HP"
   },
   "source": [
    "You can compute the metrics by calling the same functions as before. You will get an MAE close to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hR2BO0Dai_ZT"
   },
   "outputs": [],
   "source": [
    "# Compute the metrics\n",
    "print(tf.keras.metrics.mean_squared_error(x_valid, results).numpy())\n",
    "print(tf.keras.metrics.mean_absolute_error(x_valid, results).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_4pUcF0Y3bY"
   },
   "source": [
    "## Wrap Up\n",
    "\n",
    "In this lab, you were able to build and train a single layer neural network on time series data. You prepared data windows, fed them to the model, and the final predictions show comparable results with the statistical analysis you did in Week 1. In the next labs, you will try adding more layers and will also look at some optimizations you can make when training your model."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "C4_W2_Lab_2_single_layer_NN.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
